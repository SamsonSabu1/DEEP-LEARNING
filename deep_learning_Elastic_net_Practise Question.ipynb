{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f786f796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [1.0, 1.0, 1.0]\n",
      "Trained biases: [-0.5, -0.5, -0.4750786091996593]\n",
      "Output after training: 0.5031249593105316\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(input, weights, biases):\n",
    "    # Input for the first hidden layer\n",
    "    z1 = (input * weights[0]) + biases[0]\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # Input for the second hidden layer\n",
    "    z2 = (a1 * weights[1]) + biases[1]\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Input for the output layer\n",
    "    z3 = (a2 * weights[2]) + biases[2]\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a3\n",
    "\n",
    "# Gradient descent and training\n",
    "def train(input, target_output, weights, biases, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        # Forward pass\n",
    "        output = forward_pass(input, weights, biases)\n",
    "        \n",
    "        # Calculate error\n",
    "        error = target_output - output\n",
    "        \n",
    "        # Backward pass and weight updates\n",
    "        weights[2] += learning_rate * error * output * (1 - output) * weights[1] * (1 - weights[1]) * biases[0]\n",
    "        biases[2] += learning_rate * error * output * (1 - output)\n",
    "        \n",
    "        weights[1] += learning_rate * error * output * (1 - output) * weights[0] * (1 - weights[0]) * biases[1]\n",
    "        biases[1] += learning_rate * error * output * (1 - output) * weights[1] * (1 - weights[1])\n",
    "        \n",
    "        weights[0] += learning_rate * error * output * (1 - output) * input * (1 - input) * biases[0] * weights[1] * (1 - weights[1]) * biases[2] * (1 - biases[2])\n",
    "        biases[0] += learning_rate * error * output * (1 - output) * input * (1 - input) * biases[0] * weights[1] * (1 - weights[1]) * biases[2] * (1 - biases[2])\n",
    "    \n",
    "    return weights, biases, output\n",
    "\n",
    "# Initial weights and biases\n",
    "weights = [1, 1, 1]\n",
    "biases = [-0.5, -0.5, -0.5]\n",
    "\n",
    "# Input and target output\n",
    "input = 0.5\n",
    "target_output = 1\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "iterations = 2\n",
    "\n",
    "# Train the neural network\n",
    "trained_weights, trained_biases, output = train(input, target_output, weights, biases, learning_rate, iterations)\n",
    "\n",
    "print(\"Trained weights:\", trained_weights)\n",
    "print(\"Trained biases:\", trained_biases)\n",
    "print(\"Output after training:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f6677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [1.0015667775455506, 1.0062701816516397, 1.01246999256132]\n",
      "Trained biases: [-0.49686644490889903, -0.4874657736050667, -0.4751117615244141]\n",
      "Output after training: 0.5044315321365773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(input, weights, biases):\n",
    "    # Calculate input for the first hidden layer\n",
    "    z1 = (input * weights[0]) + biases[0]\n",
    "    a1 = sigmoid(z1)\n",
    "\n",
    "    # Calculate input for the second hidden layer\n",
    "    z2 = (a1 * weights[1]) + biases[1]\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Calculate input for the output layer\n",
    "    z3 = (a2 * weights[2]) + biases[2]\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, a2, a3\n",
    "\n",
    "# Backward pass with gradient descent and training\n",
    "def train(input, target_output, weights, biases, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        a1, a2, output = forward_pass(input, weights, biases)\n",
    "\n",
    "        # Calculate error\n",
    "        error = target_output - output\n",
    "\n",
    "        # Update output layer weights and bias\n",
    "        weights[2] += learning_rate * error * output * (1 - output) * a2\n",
    "        biases[2] += learning_rate * error * output * (1 - output) * 1\n",
    "\n",
    "        # Update second hidden layer weights and bias\n",
    "        weights[1] += learning_rate * error * output * (1 - output) * weights[2] * (1 - a2) * a1\n",
    "        biases[1] += learning_rate * error * output * (1 - output) * weights[2] * (1 - a2)\n",
    "\n",
    "        # Update first hidden layer weights and biases\n",
    "        weights[0] += learning_rate * error * output * (1 - output) * weights[2] * (1 - a2) * a1 * (1 - a1) * input\n",
    "        biases[0] += learning_rate * error * output * (1 - output) * weights[2] * (1 - a2) * a1 * (1 - a1)\n",
    "\n",
    "    return weights, biases, output\n",
    "\n",
    "# Initial weights and biases\n",
    "weights = [1, 1, 1]\n",
    "biases = [-0.5, -0.5, -0.5]\n",
    "\n",
    "# Input and target output\n",
    "input = 0.5\n",
    "target_output = 1\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "iterations = 2\n",
    "\n",
    "# Train the neural network\n",
    "trained_weights, trained_biases, output = train(input, target_output, weights, biases, learning_rate, iterations)\n",
    "\n",
    "print(\"Trained weights:\", trained_weights)\n",
    "print(\"Trained biases:\", trained_biases)\n",
    "print(\"Output after training:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff2388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\samso\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f7fd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 - 0s - loss: 0.2500 - 456ms/epoch - 456ms/step\n",
      "Epoch 2/2\n",
      "1/1 - 0s - loss: 0.2492 - 16ms/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x208f87752d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the neural network\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(1,), use_bias=True),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', use_bias=True)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Initialize weights and biases\n",
    "model.layers[0].set_weights([np.array([[1.]]), np.array([-0.5])])\n",
    "model.layers[1].set_weights([np.array([[1.]]), np.array([-0.5])])\n",
    "\n",
    "# Input and target output\n",
    "input_data = np.array([0.5])\n",
    "target_output = np.array([1])\n",
    "\n",
    "# Train the model for 2 iterations\n",
    "model.fit(input_data, target_output, epochs=2, batch_size=1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf05902",
   "metadata": {},
   "outputs": [],
   "source": [
    "The loss value indicates the mean squared error between the predicted output and the target output. Its a good sign that the loss decreased slightly from the first epoch to the second, indicating that the model is making progress in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5824bb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularization Loss: -0.4\n",
      "L2 Regularization Loss: -0.475\n",
      "Elastic Net Regularization Loss: -0.375\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given values\n",
    "input_vector = np.array([1, 2, 2, 1])\n",
    "bias = 0\n",
    "learned_parameters = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "expected_output = 2\n",
    "regularization_parameter_l1 = 0.1 # Example value for lambda in L1 regularization\n",
    "regularization_parameter_l2 = 0.1 # Example value for lambda in L2 regularization\n",
    "\n",
    "# Calculate the loss\n",
    "loss = np.sum((input_vector * learned_parameters) + bias) - expected_output\n",
    "\n",
    "# L1 Regularization\n",
    "l1_regularization_loss = loss + regularization_parameter_l1 * np.sum(np.abs(learned_parameters))\n",
    "\n",
    "# L2 Regularization\n",
    "l2_regularization_loss = loss + regularization_parameter_l2 * np.sum(learned_parameters ** 2)\n",
    "\n",
    "# Elastic Net Regularization\n",
    "elastic_net_regularization_loss = loss + regularization_parameter_l1 * np.sum(np.abs(learned_parameters)) + regularization_parameter_l2 * np.sum(learned_parameters ** 2)\n",
    "\n",
    "# Comparing the effects of L1, L2, and Elastic Net regularization\n",
    "print(\"L1 Regularization Loss:\", l1_regularization_loss)\n",
    "print(\"L2 Regularization Loss:\", l2_regularization_loss)\n",
    "print(\"Elastic Net Regularization Loss:\", elastic_net_regularization_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c473043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Regularization Loss: 0.09750000000000003\n",
      "L2 Regularization Loss: 0.03921184197627685\n",
      "Elastic Net Regularization Loss: 0.14629617841600068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "import numpy as np\n",
    "\n",
    "# Given values\n",
    "X = np.array([[1, 2, 2, 1]])  # input vector\n",
    "y = np.array([2])  # expected output\n",
    "reg_param_l1 = 0.1  # Example value for lambda in L1 regularization\n",
    "reg_param_l2 = 0.1  # Example value for lambda in L2 regularization\n",
    "reg_param_elastic = 0.1  # Example value for lambda in Elastic Net regularization\n",
    "bias = 0\n",
    "\n",
    "# Fit a linear regression model without regularization to obtain the learned parameters\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "learned_parameters = model.coef_\n",
    "\n",
    "# Calculate the loss\n",
    "loss = np.sum((X.dot(learned_parameters) + bias) - y) ** 2\n",
    "\n",
    "# L1 Regularization\n",
    "lasso = Lasso(alpha=reg_param_l1, fit_intercept=False)\n",
    "lasso.fit(X, y)\n",
    "total_loss_l1 = loss + reg_param_l1 * np.sum(np.abs(lasso.coef_))\n",
    "\n",
    "# L2 Regularization\n",
    "ridge = Ridge(alpha=reg_param_l2, fit_intercept=False)\n",
    "ridge.fit(X, y)\n",
    "total_loss_l2 = loss + reg_param_l2 * np.sum(ridge.coef_ ** 2)\n",
    "\n",
    "# Elastic Net Regularization\n",
    "elastic_net = ElasticNet(alpha=reg_param_elastic, l1_ratio=0.5, fit_intercept=False)\n",
    "elastic_net.fit(X, y)\n",
    "total_loss_elastic_net = loss + reg_param_elastic * (\n",
    "    np.sum(np.abs(elastic_net.coef_)) + np.sum(elastic_net.coef_ ** 2)\n",
    ")\n",
    "\n",
    "# Comparing the effects of L1, L2, and Elastic Net regularization\n",
    "print(\"L1 Regularization Loss:\", total_loss_l1)\n",
    "print(\"L2 Regularization Loss:\", total_loss_l2)\n",
    "print(\"Elastic Net Regularization Loss:\", total_loss_elastic_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "These results showcase the impact of L1, L2, and Elastic Net regularization techniques on the total loss.\n",
    "L1 regularization tends to enforce sparsity in the learned parameters, L2 regularization penalizes large weights,\n",
    "and Elastic Net is a combination of both L1 and L2 regularization, allowing for a balance between the two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbab347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKPROPAGATION IN NEURAL NETWORKS EXPLAINED\n",
    "Backpropagation is a process involved in training a neural network. \n",
    "It involves taking the error rate of a forward propagation\n",
    "and feeding this loss backward through the neural network layers to fine-tune the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28399968",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is an activation function and why use them? \n",
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it.\n",
    "The purpose of the activation function is to introduce non-linearity into the output of a neuron. \n",
    "\n",
    "Explanation:\n",
    "We know, the neural network has neurons that work in correspondence with weight, bias, and their respective activation function.\n",
    "In a neural network, we would update the weights and biases of the neurons on the basis of the error at the output. This process is known as back-propagation. Activation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. \n",
    "\n",
    "Why do we need Non-linear activation function?\n",
    "A neural network without an activation function is essentially just a linear regression model.\n",
    "The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "The sigmoid is a mathematical function that maps input values to a value between 0 and 1, \n",
    "making it useful for binary classification and logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58186717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight in a neural network refers to the numerical value assigned to the connections between the neurons in a layer. \n",
    "#These weights determine the strength of the connection between neurons and can be adjusted during the training process to optimize the network's performance.\n",
    "#The weights can take on any real value, but are often initialized with small random values.\n",
    "#The weights are typically adjusted using gradient descent, a technique that reduces the error of the network by adjusting the weights in the direction of the negative gradient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23d210fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network bias can be defined as the constant which is added to the product of features and weights. \n",
    "#It is used to offset the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cde72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The elastic net is a linear regression regularization technique that combines both the L1 (Lasso) and L2 (Ridge) regularization penalties. \n",
    "#It is particularly useful when dealing with datasets that have high collinearity or when there are more predictors than observations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
